{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "for_custom_clf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnz5MAGcPfMX"
      },
      "source": [
        "# install requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIGTLg9WKCHX",
        "outputId": "5931b85a-8f99-4fb4-e569-32850b029e77"
      },
      "source": [
        "!pip install kss"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kss\n",
            "  Downloading kss-3.1.0.4.tar.gz (42.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.3 MB 77 kB/s \n",
            "\u001b[?25hCollecting emoji\n",
            "  Downloading emoji-1.4.2.tar.gz (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 48.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kss, emoji\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-3.1.0.4-py3-none-any.whl size=42336591 sha256=f0431199dc06768db3948294e184f27a39868a3fac80923b75224c49db1d0e0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/94/d8/3c/b5f02f814e08c3e2f35e32ae2ac92a34c8412ed6f92ff470ce\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.4.2-py3-none-any.whl size=186469 sha256=a9cf33b96a259aa4f1442dbd7db33053332b4b50a265c758c52603945599eff0\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/61/e7/2fc1ac8f306848fc66c6c013ab511f0a39ef4b1825b11363b2\n",
            "Successfully built kss emoji\n",
            "Installing collected packages: emoji, kss\n",
            "Successfully installed emoji-1.4.2 kss-3.1.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NVi1r2iLQhB",
        "outputId": "09c866f7-3e69-4349-dbb4-cda4122e48c3"
      },
      "source": [
        "!pip install transformers==3.3.0"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.3.0\n",
            "  Downloading transformers-3.3.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 27.3 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 26.6 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40 kB 18.8 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 112 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 122 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 133 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 143 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 153 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 163 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 174 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 184 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 194 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 204 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 215 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 225 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 235 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 245 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 256 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 266 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 276 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 286 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 296 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 307 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 317 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 337 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 348 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 358 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 368 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 378 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 389 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 399 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 409 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 419 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 430 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 440 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 450 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 460 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 471 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 481 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 491 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 501 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 512 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 522 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 532 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 542 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 552 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 563 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 573 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 583 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 593 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 604 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 614 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 624 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 634 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 645 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 655 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 665 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 675 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 686 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 696 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 706 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 716 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 727 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 737 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 747 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 757 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 768 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 778 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 788 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 798 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 808 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 819 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 829 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 839 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 849 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 860 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 870 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 880 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 890 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 901 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 911 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 921 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 931 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 942 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 952 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 962 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 972 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 983 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 993 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.0 MB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.0 MB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.0 MB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.0 MB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0 MB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1 MB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (4.62.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (0.0.45)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 50.4 MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc2\n",
            "  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 50.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.3.0) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.0) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.0) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.0) (1.15.0)\n",
            "Installing collected packages: tokenizers, sentencepiece, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.10.3\n",
            "    Uninstalling tokenizers-0.10.3:\n",
            "      Successfully uninstalled tokenizers-0.10.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.9.2\n",
            "    Uninstalling transformers-4.9.2:\n",
            "      Successfully uninstalled transformers-4.9.2\n",
            "Successfully installed sentencepiece-0.1.96 tokenizers-0.8.1rc2 transformers-3.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INcxt8lyLYBX",
        "outputId": "38ae9461-6c5c-4cbe-cff2-651c11b37232"
      },
      "source": [
        "!pip install mxnet gluonnlp"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.7/dist-packages (1.8.0.post0)\n",
            "Collecting gluonnlp\n",
            "  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n",
            "\u001b[K     |████████████████████████████████| 344 kB 8.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (2.23.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from mxnet) (0.8.4)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (1.19.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (0.29.24)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (21.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp) (2.4.7)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595734 sha256=c4d191656126c45c52ac048dfffd9f2c90d9eec47dc75ffeee540f4924ee011c\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: gluonnlp\n",
            "Successfully installed gluonnlp-0.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ytMw2oHPiqR"
      },
      "source": [
        "# load kobert model - pretrained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fED-T3h0K4Xx",
        "outputId": "b9d35afc-fcf3-489c-c48d-623524a20195"
      },
      "source": [
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-lji4w2ia\n",
            "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-lji4w2ia\n",
            "Building wheels for collected packages: kobert\n",
            "  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert: filename=kobert-0.1.2-py3-none-any.whl size=12771 sha256=8df90e41d747617074770c0102e36fd613cae8df70027d47d62bb895efcfb827\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gk1smunp/wheels/d3/68/ca/334747dfb038313b49cf71f84832a33372f3470d9ddfd051c0\n",
            "Successfully built kobert\n",
            "Installing collected packages: kobert\n",
            "Successfully installed kobert-0.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G47kaAr0JGf5",
        "outputId": "8515f15e-7507-4e05-bfce-26d50a8ace7a"
      },
      "source": [
        "cd /content/drive/MyDrive/bert"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1nWheKWdEjM291jMl9V_KokMTeNb7xTjM/bert\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfkfMQjaIa7W"
      },
      "source": [
        "import torch\n",
        "import re\n",
        "import kss\n",
        "import pandas as pd\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R15gbJc5JXBN"
      },
      "source": [
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0I8k1t4LGYX",
        "outputId": "8b003890-e8be-4221-a187-9876df304284"
      },
      "source": [
        "bertmodel, vocab = get_pytorch_kobert_model()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[██████████████████████████████████████████████████]\n",
            "[██████████████████████████████████████████████████]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYq1jUZBOnHe"
      },
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=4,\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD3XlILiI7qL",
        "outputId": "b1a97fff-c187-431e-f947-f0cf3ab35120"
      },
      "source": [
        "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
        "model.load_state_dict(torch.load('nlp_checkpoint.pt'))\n",
        "model.eval()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERTClassifier(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qdunK6VPrY5"
      },
      "source": [
        "# predict custom fairytale text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiS-Z_zbJyuS"
      },
      "source": [
        "def cleaning(sent):\n",
        "  sent_clean = re.sub(\"[\\t\\n]\", \"\", sent)\n",
        "  sent_clean = re.sub(\"[^가-힣ㄱ-하-ㅣ!.,?\\\\d\\\\s]\", \"\", sent_clean)\n",
        "  sent_clean = re.sub(\"\\s{2,}\", \" \", sent_clean)\n",
        "  return sent_clean"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKUeoTHcJgSx"
      },
      "source": [
        "with open(\"redhat_text.txt\", \"r\") as f:  ## you can load your own fairytale text data\n",
        "    data = f.read()"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "LX2fK9eUJr4z",
        "outputId": "519b3b9b-f1f1-42b8-d466-a38114d9513f"
      },
      "source": [
        "data = cleaning(data)\n",
        "data"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'옛날 옛날에 모두의 사랑을 받는 작고 귀여운 소녀가 있었습니다. 하지만 그 소녀를 가장 사랑하는 것은 그녀의 할머니였습니다. 할머니는 소녀에게 무엇을 줘야 할지 몰랐습니다. 한번은 할머니가 소녀에게 붉은 벨벳으로 만들어진 모자를 선물했습니다. 소녀에게 그 모자가 잘 어울렸고, 소녀가 그 모자가 아닌 다른 것은 쓰지 않으려고 했습니다. 그래서 그 소녀는 빨간 모자라고 불렸습니다. 어느 날, 소녀의 엄마가 그녀에게 말했습니다. 빨간 모자, 여기로 와보렴. 여기 케이크 한 조각과 와인 한 병을 할머니에게 가져다 주렴. 할머니가 편찮으시니까 네가 가면 기뻐하실 거야. 더워지기 전에 출발하렴. 그리고 할머니 댁에 갈 때, 조심해서 가고 길에서 벗어나지 마렴. 그렇지 않으면 네가 넘어져서 병을 깨뜨릴 거야. 그러면 할머니는 아무것도 받지 못하신단다. 그리고 할머니 방에 가면, 먼저 인사하고 방 안 구석구석을 살펴보는 것을 잊지 말아라!엄마 말 잘 들을게요. 빨간 머리가 엄마에게 말했습니다. 하지만 할머니는 마을에서 30분 떨어진 거리에 있는 숲에 사셨습니다. 빨간 모자가 숲에 같을 때, 그녀는 늑대를 만났습니다. 하지만 빨간 모자는 늑대가 얼마나 나쁜 동물인지 몰랐고 늑대를 무서워하지 않았습니다. 안녕, 빨간 모자! 늑대가 말했습니다. 고마워, 늑대! 빨간 모자야, 이렇게 일찍 어딜 가니? 할머니께 무엇을 가져가니? 케이크와 와인. 어제 우리가 편찮으신 할머니께 좋은 것을 드리고 기운 내게 해드리려고 케이크를 만들었어. 빨간 모자야, 너희 할머니는 어디 사시니? 여기서도 15분 더 숲 속으로 가야 해. 3개의 오크 나무 아래에 할머니 집이 있어. 그 아래에는 개암나무가 있어. 너도 거기를 알거야. 빨간 모자가 말했습니다. 늑대는 혼자 생각했습니다. 저 어리고 상냥한 한입거리가 노인보다 훨씬 맛있을거야. 둘 다 잡아먹으려면 빨리 꾀를 생각해내야 해. 늑대가 빨간 모자 옆으로 가서 말했습니다. 빨간 모자야, 여기 주변에 있는 예쁜 꽃들 좀 봐. 왜 주변을 둘러보지 않니? 내 생각엔, 너가 새들이 얼마나 아름답게 노래하고 있는지 듣지 않는 것 같아. 너는 학교 가는 것처럼 바쁘게 가는구나. 여기 숲을 둘러보는 건 정말 재미있어.빨간 모자가 눈을 뜨고 햇빛이 나무를 향해 춤추는 모습과 예쁜 꽃들을 둘러봤을 때, 그녀는 생각했습니다. 내가 할머니께 꽃다발을 가져다 드리면, 할머니가 기뻐하실거야. 어쨌든 지금은 아직 이르니까 난 제시간에 할머니 댁에 도착할거야. 그래서 그녀는 길에서 벗어나 꽃을 찾아 다녔습니다. 그녀가 꽃 하나를 꺾을 때마다, 그녀는 조금만 더 가면 더 예쁜 꽃이 있을 거라고 생각했습니다. 그래서 계속해서 숲 속 깊이 들어갔습니다. 하지만 늑대는 바로 할머니 댁을 향해 갔고 문을 두드렸습니다. 밖에 누구세요? 빨간 모자에요. 제가 케이크와 와인을 들고 왔어요. 문 열어 주세요! 그냥 손잡이를 누르면 된단다! 할머니가 말했습니다. 나는 너무 아파서 일어날 수가 없단다. 늑대가 손잡이를 누르고 할머니 댁으로 들어갔습니다. 말도 없이 할머니 침대로 가서 할머니를 잡아먹었습니다. 그리고 늑대는 할머니의 옷을 입고, 두건을 쓰고 침대에 누워 커튼을 쳤습니다.하지만 빨간 모자는 계속 꽃을 찾아 다녔습니다. 그녀가 더 가져갈 수 없을 만큼 꽃을 모은 후에야, 할머니가 생각나서 할머니 댁으로 향하기 시작했습니다. 빨간 모자가 할머니 댁에 도착했을 때, 문이 열려 있어서 놀랐습니다. 할머니 방으로 들어갔을 때, 모든 것이 너무 이상해서 빨간 모자는 생각했습니다. 맙소사. 내가 왜 이렇게 무서워하지? 나는 할머니 댁에 있는 것을 좋아하잖아! 빨간 모자가 말했습니다. 안녕하세요. 하지만 대답은 듣지 못했습니다. 빨간 모자가 할머니 침대로 다가가서 커튼을 젖혔습니다. 거기에 할머니가 두건을 깊게 눌러쓰고 누워있었고 매우 이상해 보였습니다. 아, 할머니, 귀가 정말 크시네요! 내가 너의 목소리를 더 잘 듣기 위해서란다! 아, 할머니, 눈이 정말 크시네요! 내가 너를 더 잘 보기 위해서란다! 아, 할머니, 손이 정말 크시네요! 내가 너를 더 잘 잡기 위해서란다! 하지만, 할머니, 무서울 만큼 큰 입을 가지셨네요! 내가 너를 더 잘 잡아먹기 위해서란다! 늑대가 그것을 말하자마자 침대에서 뛰어나와 그 불쌍한 빨간 모자를 잡아먹었습니다.늑대가 다 먹어 치웠을 때, 그는 다시 침대에 누워서 잠에 들었고 시끄럽게 코를 골기 시작했습니다. 한 사냥꾼이 할머니 댁을 지나가면서 생각했습니다. 할머니가 왜 이렇게 시끄럽게 코를 고시는 거지? 가서 할머니에게 문제가 있나 확인해봐야겠다. 그래서 그는 할머니 방 안으로 들어갔습니다. 그가 침대로 왔을 때, 그는 늑대가 거기에 누워있는 것을 봤습니다. 너를 여기서 보는구나 사냥꾼이 말했습니다. 내가 너를 오랫동안 찾아다녔지. 이제 그는 늑대에게 총을 쏘려고 했습니다. 그런데 그때 그에게 무언가 떠올랐습니다. 늑대가 할머니를 잡아먹었지만, 할머니를 구할 수 있을지도 몰라. 총을 쏘면 안돼. 그 대신에 사냥꾼은 가위를 가져와서 자고 있는 늑대의 배를 가르기 시작했습니다. 잠시 후에 그는 빨간 모자를 발견했습니다. 그리고 늑대 배를 조금 더 자르자 빨간 모자가 나와서 말했습니다. 저는 정말 무서웠어요. 늑대 배 안은 정말 어두웠어요! 그리고 할머니도 살아서 나왔고 가까스로 숨을 쉬었습니다.빨간 모자가 무거운 돌들을 가져와서 늑대의 배 안을 그 돌로 채웠습니다. 그래서 늑대가 깨어났을 때, 그가 도망가려고 해도 돌이 너무 무거워서 넘어져 죽을 것입니다. 그 세 명은 모두 행복했습니다. 사냥꾼은 늑대의 가죽을 가져갔습니다. 할머니는 빨간 모자가 가져 온 먹고 와인을 마시고 병에서 회복했습니다. 빨간 모자는 생각했습니다. 내가 살아있는 동안 엄마가 허락하지 않으시면 다시는 혼자 길에서 벗어나지 않을 거야.이 이야기도 말해야 합니다. 빨간 모자가 할머니께 다시 케이크를 드리러 갈 때, 또 다른 늑대가 와서 빨간 모자에게 말을 시키고 빨간 모자가 길에서 벗어나게 하려고 했습니다. 하지만 빨간 모자는 늑대를 경계하며 바로 할머니 댁으로 갔습니다. 빨간 모자가 할머니께 자기에게 인사를 했지만 자기를 째려보는 늑대를 만났다고 말했습니다. 내가 사람들이 있는 거리에 있지 않았다면, 늑대가 나를 잡아먹었을 거에요. 이리 와라 할머니가 말씀하셨습니다. 우리 늑대가 들어오지 못하게 문을 닫자. 곧 늑대가 와서 문을 두드리며 말했습니다. 할머니, 문 열어주세요. 저에요, 빨간 모자. 제가 할머니께 케이크를 드리러 왔어요. 하지만 할머니는 아무 말도 하지 않고 문을 열어주지 않았습니다. 그랬더니 그 나쁜 늑대가 계속 할머니 집 주위를 걸어 다니더니 마침내 지붕 위로 뛰어 올랐습니다. 늑대는 빨간 모자가 저녁에 집에 갈 때까지 기다려 빨간 모자를 따라가서 어둠 속에서 몰래 잡아먹으려고 했습니다. 하지만 할머니는 늑대가 무엇을 하려고 하는지 알았습니다. 집 앞에 큰 양동이가 있었습니다. 할머니는 빨간 모자에게 말했습니다. 빨간 모자야, 저기서 양동이를 가져와라. 내가 어제 거기에 소시지를 구웠다. 그 양동이에 물을 부어라! 빨간 모자는 양동이가 완전히 찰 때까지 물을 채웠습니다. 그래서 소시지 냄새가 늑대의 코까지 올라갔습니다. 늑대는 냄새를 맡고 아래를 내려봤습니다. 결국 늑대가 너무 목을 아래로 내린 나머지, 스스로 지탱할 수 없어서 아래로 떨어졌습니다. 늑대는 굴뚝으로 미끄러져 내려와 양동이에 빠져 죽었습니다. 그리고 빨간 모자는 행복하고 안전하게 집에 돌아왔습니다.'"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-1R1m71J4Lc",
        "outputId": "50823a13-8864-4363-c645-070f42e4e0f6"
      },
      "source": [
        "new_datalines = kss.split_sentences(data)\n",
        "len(new_datalines)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drFOeJfCKMVS"
      },
      "source": [
        "pred = pd.DataFrame({\"contents\": new_datalines})"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUr_oFz1KOKp",
        "outputId": "82766ea9-0886-4bd7-96f4-00482f6fb389"
      },
      "source": [
        "#토큰화\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gEeVFDPNFjF"
      },
      "source": [
        "## Setting parameters\n",
        "max_len = 39\n",
        "batch_size = 32"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toQJyGmdM4z6"
      },
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE8zLPKnMnt_"
      },
      "source": [
        "def predict(predict_sentence):\n",
        "    sentiment_list = []\n",
        "\n",
        "    for sentence in predict_sentence:\n",
        "        data = [sentence, '0']\n",
        "        dataset_another = [data]\n",
        "\n",
        "        another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
        "        test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=4)\n",
        "        \n",
        "        model.eval()\n",
        "        for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "            token_ids = token_ids.long().to(device)\n",
        "            segment_ids = segment_ids.long().to(device)\n",
        "            valid_length= valid_length\n",
        "            label = label.long().to(device)\n",
        "            out = model(token_ids, valid_length, segment_ids)\n",
        "            test_eval=[]\n",
        "\n",
        "            for i in out:\n",
        "                logits=i\n",
        "                logits = logits.detach().cpu().numpy()\n",
        "                test_eval.append(np.argmax(logits))\n",
        "\n",
        "            sentiment_list.append(test_eval[0])\n",
        "\n",
        "    return sentiment_list"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwrdQhGBMuRv"
      },
      "source": [
        "sents = list(pred['contents'])\n",
        "sentiment_list = predict(sents)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "hNJRrN6LMvt6",
        "outputId": "b32d0959-081d-4e32-d904-7276a293869c"
      },
      "source": [
        "pred['sentiment'] = sentiment_list\n",
        "pred.head()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>contents</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>옛날 옛날에 모두의 사랑을 받는 작고 귀여운 소녀가 있었습니다. 하지만 그 소녀를 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>할머니는 소녀에게 무엇을 줘야 할지 몰랐습니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>한번은 할머니가 소녀에게 붉은 벨벳으로 만들어진 모자를 선물했습니다.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>소녀에게 그 모자가 잘 어울렸고, 소녀가 그 모자가 아닌 다른 것은 쓰지 않으려고 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>그래서 그 소녀는 빨간 모자라고 불렸습니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            contents  sentiment\n",
              "0  옛날 옛날에 모두의 사랑을 받는 작고 귀여운 소녀가 있었습니다. 하지만 그 소녀를 ...          1\n",
              "1                         할머니는 소녀에게 무엇을 줘야 할지 몰랐습니다.          0\n",
              "2             한번은 할머니가 소녀에게 붉은 벨벳으로 만들어진 모자를 선물했습니다.          1\n",
              "3  소녀에게 그 모자가 잘 어울렸고, 소녀가 그 모자가 아닌 다른 것은 쓰지 않으려고 ...          1\n",
              "4                           그래서 그 소녀는 빨간 모자라고 불렸습니다.          0"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m08Om9VOPAs0"
      },
      "source": [
        "# Save csv\n",
        "pred.to_csv('0827_redhat_onlytales.csv', encoding='utf-8-sig', index=True)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cdXZumzPcQ7"
      },
      "source": [
        "# final output for Tacotron2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZEW2mdePKWC"
      },
      "source": [
        "prefix_ref = \"/content/drive/MyDrive/emotion-tts/jvoice/jvoice_3_L.wav\"  ## get your own voice reference wav directory\n",
        "speaker_id = \"pjh\" ## your speaker_id\n",
        "tts_text = \"\"\n",
        "\n",
        "for index, row in pred.iterrows():\n",
        "  tts_text += prefix_ref + \"|\" + row['contents'] + \"|\"  + speaker_id + \"|\" + str(row['sentiment']) + \"|\" + str(row['sentiment'])\n",
        "  tts_text += \"\\n\""
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JiLCBGxPTi1"
      },
      "source": [
        "with open('bert_to_tts_redhat.txt', 'w', encoding='utf-8')as f:\n",
        "  f.write(tts_text)"
      ],
      "execution_count": 67,
      "outputs": []
    }
  ]
}